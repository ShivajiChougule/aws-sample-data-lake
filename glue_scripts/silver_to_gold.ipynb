{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n%connections glue_delta_connection\n%additional_python_modules delta-spark\n%extra_py_files s3://my-python-demo-jars/delta-core_2.12-2.1.0.jar,s3://my-python-demo-jars/delta-spark_2.13-3.1.0.jar,s3://my-python-demo-jars/delta-storage-3.1.0.jar\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Connections to be included:\nglue_delta_connection\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Additional python modules to be included:\ndelta-spark\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Extra py files to be included:\ns3://my-python-demo-jars/delta-core_2.12-2.1.0.jar\ns3://my-python-demo-jars/delta-spark_2.13-3.1.0.jar\ns3://my-python-demo-jars/delta-storage-3.1.0.jar\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n     \"conf\": {         \n        \"spark.jars.packages\": \"io.delta:delta-core_2.12-2.1.0.jar\",\n        \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n        \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n         \"spark.delta.logStore.class\":\"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\"\n    },\n    \"datalake-formats\":\"delta\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1c738945-de6d-4464-bed7-ae132f8a22a9.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'conf': {'spark.jars.packages': 'io.delta:delta-core_2.12-2.1.0.jar', 'spark.sql.extensions': 'io.delta.sql.DeltaSparkSessionExtension', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.delta.catalog.DeltaCatalog', 'spark.delta.logStore.class': 'org.apache.spark.sql.delta.storage.S3SingleDriverLogStore'}, 'datalake-formats': 'delta'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom awsglue.dynamicframe import DynamicFrame\nimport boto3\nfrom delta import *\n\n\ndef parse_s3_path(path):\n    parts = path.split('/')\n    bucket_name = parts[2]\n    prefix = '/'.join(parts[3:])\n    return bucket_name, prefix\n\n\ndef s3_object_cnt(path):\n\n    s3_client = boto3.client('s3')\n    bucket_name, prefix = parse_s3_path(path)\n    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n    \n    if len(response['Contents']) < 2 and response['Contents'][0]['Size'] == 0:\n        cnt = 0\n        print(\"count is\",cnt)\n    else:\n        cnt = 1\n        \n    print(f'Object count in {bucket_name} and {prefix} prefix is {cnt}')\n    return cnt\n\n\ndef add_surrogate_key(df: DataFrame, name: str):\n    \"\"\"\n    This function generates a surrogate key using a combination of column values and current timestamp.\n    \"\"\"\n    timestamp_column = current_timestamp()    \n    unique_key_column = lit(str(uuid.uuid4()))\n    surrogate_key_column = sha2(concat_ws(\"_\", *df.columns, timestamp_column, unique_key_column), 256)\n\n    \n\n\ndef create_delta_tbl(df, path):  \n    \n    df.write.format(\"delta\").mode(\"append\").save(path)\n    \ndef create_stage_delta_tbl(df, path):\n    \"\"\"\n    This function will overwrites exsiting data in s3 prefix and write dataframe to S3 in deltalake format.\n    \"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").option(\n        \"mergeSchema\", \"true\").save(path)\n    return DeltaTable.forPath(spark, path)       \n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.window import Window\ndef employee_capture_changes(stage_tbl, base_tbl):\n    \"\"\"\n    This function will identify chnages between source data and target deltalake dataset.\n\n    \"\"\"\n    updates_df = stage_tbl.toDF().alias(\"stage\")\\\n        .join(base_tbl.alias(\"base\"), col(\"stage.customer_id\") == col(\"base.customer_id\"), \"left\")\\\n        .filter((col('base.Customer_SurrogateKey').isNull()) | (col('base.Customer_SurrogateKey') != col('stage.Customer_SurrogateKey')))\n    \n\n    cols = ['customer_id', 'first_name', 'last_name', 'city',\n            'phone', 'state', 'Customer_SurrogateKey','zip','customer_class','email','delete_flag']\n    stage_updates = updates_df.select(\n        'stage.*').withColumn(\"delete_flag\", lit(False)).select(*cols)\n    base_updates = updates_df.select('base.*').drop('isCurrent', 'start_date', 'end_date').select(*cols)\n\n    # perform union\n    union_updates = base_updates.union(\n        stage_updates).filter(col('Customer_SurrogateKey').isNotNull())\n\n    # identify deleted records\n    base_tbl.alias('base').join(\n        stage_tbl.toDF().alias('stage'), 'customer_id', 'anti')\n\n    drop_del_cols = ['delete_flag', 'start_date', 'end_date', 'isCurrent']\n    del_df = base_tbl.alias('base')\\\n        .join(stage_tbl.toDF().alias('stage'), 'customer_id', 'anti')\\\n        .drop(*drop_del_cols)\\\n        .withColumn(\"merge_delete_flag\", lit(True))\\\n        .withColumnRenamed('merge_delete_flag', 'delete_flag')\n\n    union_updates_dels = union_updates.union(del_df)\n\n    return union_updates_dels\n\n\ndef employee_delta_records(base_tbl, union_updates_dels):\n    \"\"\"\n    This function will identify delete records and sets delete_flag to true and updates end_date.\n    \"\"\"\n    \n    delete_join_cond = \"customer.customer_id=customerUpdates.customer_id and customer.Customer_SurrogateKey = customerUpdates.Customer_SurrogateKey\"\n    delete_cond = \"customer.Customer_SurrogateKey == customerUpdates.Customer_SurrogateKey and customer.isCurrent = true and customerUpdates.delete_flag = true\"\n\n    base_tbl.alias(\"customer\").merge(union_updates_dels.alias(\"customerUpdates\"), delete_join_cond).whenMatchedUpdate(\n        condition=delete_cond, set={\"isCurrent\": \"false\", \"end_date\": current_date(), \"delete_flag\": \"true\"}).execute()\n\n\ndef employee_upsert_records(base_tbl, union_updates_dels):\n    \"\"\"\n    This function will identify insert/update records and performs merge operation on exsiting deltalake dataset.\n    \"\"\"\n    upsert_cond = \"customer.customer_id=customerUpdates.customer_id and customer.Customer_SurrogateKey = customerUpdates.Customer_SurrogateKey and customer.isCurrent = true\"\n    upsert_update_cond = \"customer.isCurrent = true and customerUpdates.delete_flag = false\"\n\n    base_tbl.alias(\"customer\").merge(union_updates_dels.alias(\"customerUpdates\"), upsert_cond).whenMatchedUpdate(condition=upsert_update_cond, set={\"isCurrent\": \"false\", \"end_date\": current_date()}).whenNotMatchedInsert(\n        values={\"isCurrent\": \"true\", \"customer_id\": \"customerUpdates.customer_id\", \"first_name\": \"customerUpdates.first_name\", \"last_name\": \"customerUpdates.last_name\", \"city\": \"customerUpdates.city\", \"phone\": \"customerUpdates.phone\",\n                \"state\": \"customerUpdates.state\", \"Customer_SurrogateKey\": \"customerUpdates.Customer_SurrogateKey\",\"zip\":\"customerUpdates.zip\",\"customer_class\":\"customerUpdates.customer_class\",\"email\":\"customerUpdates.email\", \"start_date\": current_date(), \"delete_flag\":  \"customerUpdates.delete_flag\", \"end_date\": \"null\"}).execute()\n\n\ndef employees_scd(stage_s3_path, processed_s3_path):\n    \"\"\"\n    This function will perform scd type 2 on deltalake dataset on s3 for employees sample dataset. \n    \"\"\"\n    srcDyf =  glueContext.create_dynamic_frame.from_catalog(database=\"data_lake_catalog_db\", table_name=\"silver_customers\")    \n        \n    srcDF = create_Surrogatekey(srcDyf.toDF(), 'Customer_SurrogateKey')\n    \n    df2 = srcDF.withColumn(\"rn\", row_number().over(Window.partitionBy(\"Customer_SurrogateKey\").orderBy(\"customer_id\")))\n    df3 = df2.filter(\"rn = 1\").drop(\"rn\")\n\n    if s3_object_cnt(processed_s3_path) != 0:\n        print(f'Deltalake customer data already exists. started loading data ...')\n\n        # delta lake satge table for input data source.\n        stage_tbl = create_stage_delta_tbl(df3, stage_s3_path)\n\n        # read delta lake base table for current records\n        base_tbl = DeltaTable.forPath(spark, processed_s3_path)\n\n        base_tbl_df = base_tbl.toDF().where((col('isCurrent') == \"true\")\n                                            & (col('delete_flag') == \"false\"))\n\n        union_updates_dels = employee_capture_changes(stage_tbl, base_tbl_df)\n\n        # perform soft deletes\n        employee_delta_records(base_tbl, union_updates_dels)\n\n        # refresh base table and perform merge operation\n        base_tbl = DeltaTable.forPath(spark, processed_s3_path)\n\n        employee_upsert_records(base_tbl, union_updates_dels)\n\n    else:\n        print(f'Delta lake customer data dosent exists. Performing inital data load ...')\n\n        srcDF = srcDF.withColumn(\"start_date\", current_date()).withColumn(\"end_date\", lit(None).cast(\n            StringType())).withColumn(\"isCurrent\", lit(True)).withColumn(\"delete_flag\", lit(False))\n\n        create_delta_tbl(srcDF, processed_s3_path)\n\nstage_s3_path=\"s3://data-lake-demo-stage/customer/\"\nprocessed_s3_path=\"s3://data-lake-demo-medallion/gold/customers/\"\nemployees_scd(stage_s3_path, processed_s3_path)\n   ",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "Object count in data-lake-demo-medallion and gold/customers/ prefix is 1\nDeltalake customer data already exists. started loading data ...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}